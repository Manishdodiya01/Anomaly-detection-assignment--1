{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afb40dae-4c10-4192-a5d7-c91bcc9e81e5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Q1. What is anomaly detection and what is its purpose?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88dbbf61-20e5-4daa-9ef3-bb6511b9d755",
   "metadata": {},
   "source": [
    "Anomaly detection, also known as outlier detection, is the process of identifying unexpected events, observations, or items that differ significantly from the norm. It is used in a wide variety of industries, including cybersecurity, fraud detection, healthcare, and manufacturing, to detect anomalies that may indicate problems or opportunities.\n",
    "\n",
    "Purpose of anomaly detection:\n",
    "\n",
    "Identify and prevent security threats: Anomaly detection can be used to identify suspicious activity on networks, systems, and data. This can help to prevent security breaches, data loss, and other malicious activity.\n",
    "Detect fraud: Anomaly detection can be used to detect fraudulent activity in financial transactions, insurance claims, and other types of data. This can help to reduce financial losses and protect consumers and businesses.\n",
    "Improve operational efficiency: Anomaly detection can be used to identify problems in manufacturing processes, supply chains, and other operations. This can help to improve efficiency and reduce costs.\n",
    "Identify new opportunities: Anomaly detection can be used to identify unusual patterns in data that may indicate new opportunities. For example, a retailer may use anomaly detection to identify new products that are likely to be popular with customers.\n",
    "Example of anomaly detection:\n",
    "\n",
    "A bank may use anomaly detection to identify fraudulent credit card transactions. The bank would train an anomaly detection model on historical data of normal credit card transactions. The model would learn to identify patterns in the data that are associated with normal transactions. The model would then be used to monitor new credit card transactions for any anomalies. If the model detects an anomaly, it would flag the transaction for review by a human analyst.\n",
    "\n",
    "Anomaly detection is a powerful tool that can be used to improve security, reduce fraud, improve operational efficiency, and identify new opportunities. It is an essential tool for many businesses and organizations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a293d48-bcc7-42da-89f5-87826744bef8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Q2. What are the key challenges in anomaly detection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027ff8ac-e464-4476-99f9-7e17a88b960b",
   "metadata": {},
   "source": [
    "The key challenges in anomaly detection include:\n",
    "\n",
    "Identifying anomalies in high-dimensional data: Data is often collected from a variety of sources and in a variety of formats. This can make it difficult to identify anomalies, especially in high-dimensional data.\n",
    "Dealing with concept drift: The distribution of data can change over time. This is known as concept drift. Anomaly detection models need to be able to adapt to concept drift in order to continue to be effective.\n",
    "Balancing false positives and false negatives: Anomaly detection models need to be able to balance the risk of false positives (detecting normal data as anomalous) and false negatives (failing to detect anomalous data).\n",
    "Interpreting the results of anomaly detection: Once an anomaly has been detected, it is important to be able to interpret the results and determine what, if any, action needs to be taken.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5587b3-5b72-47be-9555-057524cbe9b8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Q3. How does unsupervised anomaly detection differ from supervised anomaly detection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df3232b-d338-4c84-b507-69a208805156",
   "metadata": {},
   "source": [
    "Unsupervised anomaly detection algorithms do not require any labeled data. Instead, they learn to identify anomalies by analyzing the distribution of the data. Supervised anomaly detection algorithms, on the other hand, require labeled data, i.e., data that has been labeled as either normal or anomalous.\n",
    "\n",
    "Unsupervised anomaly detection algorithms are often used when labeled data is not available or is too expensive to collect. Supervised anomaly detection algorithms can be more accurate than unsupervised algorithms, but they require more data and can be more computationally expensive to train.\n",
    "\n",
    "Here is a table that summarizes the key differences between unsupervised and supervised anomaly detection:\n",
    "\n",
    "Characteristic\tUnsupervised anomaly detection\tSupervised anomaly detection\n",
    "Labeled data required\tNo\tYes\n",
    "Accuracy\tLower\tHigher\n",
    "Computational cost\tLower\tHigher\n",
    "Use cases\tSuitable for cases where labeled data is not available or too expensive to collect\tSuitable for cases where labeled data is available and accuracy is critical\n",
    "Examples of unsupervised anomaly detection algorithms include:\n",
    "\n",
    "Local outlier factor (LOF)\n",
    "Isolation forest\n",
    "One-class support vector machines (OC-SVMs)\n",
    "Examples of supervised anomaly detection algorithms include:\n",
    "\n",
    "Logistic regression\n",
    "Decision trees\n",
    "Support vector machines (SVMs)\n",
    "Which type of anomaly detection algorithm to use depends on the specific application and the availability of labeled data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e39e6d-8001-4d14-9413-166a2b22a643",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Q4. What are the main categories of anomaly detection algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f8691d-ed77-42e5-9ea2-d2e426b8597a",
   "metadata": {},
   "source": [
    "There are three main categories of anomaly detection algorithms:\n",
    "\n",
    "Unsupervised anomaly detection: Unsupervised anomaly detection algorithms do not require any labeled data. Instead, they learn to identify anomalies by analyzing the distribution of the data.\n",
    "Semi-supervised anomaly detection: Semi-supervised anomaly detection algorithms require a small amount of labeled data to train the model, but they can also learn from unlabeled data.\n",
    "Supervised anomaly detection: Supervised anomaly detection algorithms require labeled data, i.e., data that has been labeled as either normal or anomalous."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcae1cc4-e8de-4361-950a-dca132f85319",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Q5. What are the main assumptions made by distance-based anomaly detection methods?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce3f208-988e-4b36-8bcf-e7db4791766a",
   "metadata": {},
   "source": [
    "Distance-based anomaly detection methods assume that normal data points are close to each other in the feature space, while anomalous data points are distant from normal data points. This assumption is often violated in real-world data, which can lead to false positives and false negatives.\n",
    "\n",
    "Other assumptions made by distance-based anomaly detection methods include:\n",
    "\n",
    "The data is well-behaved, meaning that it is free from noise and outliers.\n",
    "The features are independent of each other.\n",
    "The distance metric used is appropriate for the data.\n",
    "Despite these assumptions, distance-based anomaly detection methods are still widely used because they are simple to implement and interpret.\n",
    "\n",
    "Here are some examples of distance-based anomaly detection methods:\n",
    "\n",
    "Nearest neighbor: The nearest neighbor method identifies an anomaly as a data point that is different from its nearest neighbors.\n",
    "K-nearest neighbors: The k-nearest neighbors method identifies an anomaly as a data point that is different from its k nearest neighbors.\n",
    "Local outlier factor (LOF): The LOF algorithm identifies anomalies as data points that have a high LOF score. The LOF score of a data point is a measure of how much the data point is isolated from its neighbors.\n",
    "Isolation forest: The isolation forest algorithm identifies anomalies as data points that are easily isolated from the rest of the data.\n",
    "Distance-based anomaly detection methods can be used in a variety of applications, including fraud detection, intrusion detection, and system health monitoring."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ae8462-6461-4ca1-adf6-17773f1522bf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Q6. How does the LOF algorithm compute anomaly scores?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb47b89d-7eb2-44e9-8809-5d986015cd83",
   "metadata": {},
   "source": [
    "The LOF algorithm computes anomaly scores by comparing the local density of a data point to the local density of its neighbors. The local density of a data point is measured by the average distance between the data point and its k nearest neighbors.\n",
    "\n",
    "To compute the anomaly score of a data point, the LOF algorithm first calculates the reachability distance of the data point. The reachability distance of a data point is the maximum of the distance between the data point and its kth nearest neighbor and the average distance between the data point and its k nearest neighbors.\n",
    "\n",
    "Once the reachability distance has been calculated, the LOF algorithm calculates the local reachability density (LRD) of the data point. The LRD of a data point is the reciprocal of the average reachability distance of the data point to its k nearest neighbors.\n",
    "\n",
    "Finally, the LOF algorithm calculates the anomaly score of the data point by dividing the average LRD of the data point's k nearest neighbors by the LRD of the data point itself.\n",
    "\n",
    "Data points with a high LOF score are more likely to be anomalies than data points with a low LOF score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac44b11-820b-4663-80eb-a3973262d461",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Q7. What are the key parameters of the Isolation Forest algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dafd124-9f23-4cef-adfb-5a54e83673cc",
   "metadata": {},
   "source": [
    "The Isolation Forest algorithm has two key parameters:\n",
    "\n",
    "n_estimators: The number of trees to build in the isolation forest.\n",
    "max_samples: The maximum number of data points to sample when building a tree.\n",
    "The value of n_estimators controls the accuracy and computational cost of the algorithm. A higher value of n_estimators will result in a more accurate model, but it will also be more computationally expensive to train.\n",
    "\n",
    "The value of max_samples controls the runtime of the algorithm. A higher value of max_samples will result in a faster algorithm, but it may also be less accurate.\n",
    "\n",
    "In addition to these two key parameters, the Isolation Forest algorithm also has a number of other parameters that can be tuned to improve the performance of the algorithm on a specific dataset.\n",
    "\n",
    "The Isolation Forest algorithm is a popular choice for anomaly detection because it is simple to implement and interpret, and it is relatively robust to noise and outliers. It has been used in a variety of applications, including fraud detection, intrusion detection, and system health monitoring."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ae418a-984d-46d9-aabd-7608a7651146",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Q8. If a data point has only 2 neighbours of the same class within a radius of 0.5, what is its anomaly score using KNN with K=10?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a2a76ad-787e-4189-b131-8f8df88d7730",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors , KNeighborsClassifier\n",
    "\n",
    "X = np.random.rand(100,2)\n",
    "\n",
    "# Set up the KNN model with K=10\n",
    "knn = NearestNeighbors(n_neighbors=10)\n",
    "knn.fit(X)\n",
    "distances, indices = knn.kneighbors(X)\n",
    "anomaly_scores = []\n",
    "\n",
    "for i in range(len(X)):\n",
    "    if (distances[i, 2] <= 0.5):\n",
    "        anomaly_scores.append(1)\n",
    "    else:\n",
    "        anomaly_scores.append(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e92c9d8-4c41-4565-9eeb-fdcdc3c8cfc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anomaly_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a72cd1-a73b-4947-824d-146d41eabc14",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Q9. Using the Isolation Forest algorithm with 100 trees and a dataset of 3000 data points, what is the anomaly score for a data point that has an average path length of 5.0 compared to the average path length of the trees?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5036025c-83a6-404f-b302-a56d0ff2f3ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>IsolationForest(random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">IsolationForest</label><div class=\"sk-toggleable__content\"><pre>IsolationForest(random_state=0)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "IsolationForest(random_state=0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "import numpy as np\n",
    "\n",
    "# Generate example data\n",
    "np.random.seed(0)\n",
    "X = np.random.randn(3000, 2)\n",
    "outliers = np.array([[6, 6], [7, 7], [8, 8]])\n",
    "X = np.vstack((X, outliers))\n",
    "\n",
    "clf = IsolationForest(n_estimators=100 , random_state=0)\n",
    "clf.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1dabc74-471b-44a2-863f-51da2bf1e9ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomaly Score for the New Data Point: 0.11054894663568299\n",
      "Average Path Length of the Trees: 0.4611624932333125\n"
     ]
    }
   ],
   "source": [
    "new_data_point = np.array([[0, 0]]) \n",
    "anomaly_score = clf.decision_function(new_data_point)\n",
    "avg_path_length_trees = -clf.score_samples(X).mean()\n",
    "\n",
    "print(f\"Anomaly Score for the New Data Point: {anomaly_score[0]}\")\n",
    "print(f\"Average Path Length of the Trees: {avg_path_length_trees}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31aa8653-db2b-4115-bd73-57985fc09aef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
